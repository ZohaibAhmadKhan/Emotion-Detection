{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras import layers, Model\n",
        "\n",
        "\n",
        "# loading the EMOTHAW dataset\n",
        "def load_emothaw_data():\n",
        "data, labels = load_emothaw_data()\n"
      ],
      "metadata": {
        "id": "HLE4A8iJe57z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Feature Extraction\n",
        "# EMOTHAW dataset, a 3D array (samples, sequence length, features)\n",
        "\n",
        "# Extracting time domain features\n",
        "def extract_time_domain_features(data):\n",
        "    time_features = []\n",
        "    for sample in data:\n",
        "        sample_features = []\n",
        "        for feature in sample.T:  # Assuming the features are along the last axis\n",
        "            feature_stats = [\n",
        "                np.mean(feature),\n",
        "                np.std(feature),\n",
        "                np.min(feature),\n",
        "                np.max(feature),\n",
        "                np.percentile(feature, 25),\n",
        "                np.percentile(feature, 50),\n",
        "                np.percentile(feature, 75)\n",
        "            ]\n",
        "            sample_features.extend(feature_stats)\n",
        "        time_features.append(sample_features)\n",
        "    return np.array(time_features)\n",
        "\n",
        "time_domain_features = extract_time_domain_features(data)\n",
        "\n",
        "# Extracting frequency domain features using FFT\n",
        "def extract_frequency_domain_features(data):\n",
        "    freq_features = []\n",
        "    for sample in data:\n",
        "        sample_features = []\n",
        "        for feature in sample.T:  # Assuming the features are along the last axis\n",
        "            freq_feature = np.fft.fft(feature)\n",
        "            freq_magnitude = np.abs(freq_feature)\n",
        "            dominant_freq = np.argmax(freq_magnitude)\n",
        "            freq_energy = np.sum(freq_magnitude)\n",
        "            sample_features.extend([dominant_freq, freq_energy])\n",
        "        freq_features.append(sample_features)\n",
        "    return np.array(freq_features)\n",
        "\n",
        "frequency_domain_features = extract_frequency_domain_features(data)\n",
        "\n",
        "# Extracting statistical features\n",
        "def extract_statistical_features(data):\n",
        "    statistical_features = []\n",
        "    for sample in data:\n",
        "        sample_features = []\n",
        "        for feature in sample.T:  # The features are along the last axis\n",
        "            feature_stats = [\n",
        "                np.mean(feature),\n",
        "                np.median(feature),\n",
        "                np.var(feature),\n",
        "                np.skew(feature),\n",
        "                np.kurtosis(feature)\n",
        "            ]\n",
        "            sample_features.extend(feature_stats)\n",
        "        statistical_features.append(sample_features)\n",
        "    return np.array(statistical_features)\n",
        "\n",
        "statistical_features = extract_statistical_features(data)\n"
      ],
      "metadata": {
        "id": "zw_KP_QPfsmz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_9cChuPlcAcG"
      },
      "outputs": [],
      "source": [
        "#Applying Attention-based Transformer Model\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, Model\n",
        "\n",
        "class MultiHeadSelfAttention(layers.Layer):\n",
        "    def __init__(self, embed_dim, num_heads):\n",
        "        super(MultiHeadSelfAttention, self).__init__()\n",
        "        self.embed_dim = embed_dim\n",
        "        self.num_heads = num_heads\n",
        "        if embed_dim % num_heads != 0:\n",
        "            raise ValueError(f\"embedding dimension = {embed_dim} should be divisible by number of heads = {num_heads}\")\n",
        "        self.projection_dim = embed_dim // num_heads\n",
        "        self.query_dense = layers.Dense(embed_dim)\n",
        "        self.key_dense = layers.Dense(embed_dim)\n",
        "        self.value_dense = layers.Dense(embed_dim)\n",
        "        self.combine_heads = layers.Dense(embed_dim)\n",
        "\n",
        "    def attention(self, query, key, value):\n",
        "        score = tf.matmul(query, key, transpose_b=True)\n",
        "        dim_key = tf.cast(tf.shape(key)[-1], tf.float32)\n",
        "        scaled_score = score / tf.math.sqrt(dim_key)\n",
        "        weights = tf.nn.softmax(scaled_score, axis=-1)\n",
        "        output = tf.matmul(weights, value)\n",
        "        return output, weights\n",
        "\n",
        "    def separate_heads(self, x, batch_size):\n",
        "        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.projection_dim))\n",
        "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
        "\n",
        "    def call(self, inputs):\n",
        "        batch_size = tf.shape(inputs)[0]\n",
        "        query = self.query_dense(inputs)\n",
        "        key = self.key_dense(inputs)\n",
        "        value = self.value_dense(inputs)\n",
        "        query = self.separate_heads(query, batch_size)\n",
        "        key = self.separate_heads(key, batch_size)\n",
        "        value = self.separate_heads(value, batch_size)\n",
        "        attention, weights = self.attention(query, key, value)\n",
        "        attention = tf.transpose(attention, perm=[0, 2, 1, 3])\n",
        "        concat_attention = tf.reshape(attention, (batch_size, -1, self.embed_dim))\n",
        "        output = self.combine_heads(concat_attention)\n",
        "        return output\n",
        "\n",
        "class TransformerBlock(layers.Layer):\n",
        "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n",
        "        super(TransformerBlock, self).__init__()\n",
        "        self.att = MultiHeadSelfAttention(embed_dim, num_heads)\n",
        "        self.ffn = tf.keras.Sequential(\n",
        "            [layers.Dense(ff_dim, activation=\"relu\"), layers.Dense(embed_dim),]\n",
        "        )\n",
        "        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.dropout1 = layers.Dropout(rate)\n",
        "        self.dropout2 = layers.Dropout(rate)\n",
        "\n",
        "    def call(self, inputs, training):\n",
        "        attn_output = self.att(inputs)\n",
        "        attn_output = self.dropout1(attn_output, training=training)\n",
        "        out1 = self.layernorm1(inputs + attn_output)\n",
        "        ffn_output = self.ffn(out1)\n",
        "        ffn_output = self.dropout2(ffn_output, training=training)\n",
        "        return self.layernorm2(out1 + ffn_output)\n",
        "\n",
        "class TransformerClassifier(Model):\n",
        "    def __init__(self, num_classes, embed_dim, num_heads, ff_dim, num_blocks):\n",
        "        super(TransformerClassifier, self).__init__()\n",
        "        self.num_blocks = num_blocks\n",
        "        self.embedding = layers.Dense(embed_dim)\n",
        "        self.transformer_blocks = [TransformerBlock(embed_dim, num_heads, ff_dim) for _ in range(num_blocks)]\n",
        "        self.flatten = layers.Flatten()\n",
        "        self.classifier = layers.Dense(num_classes, activation='softmax')\n",
        "\n",
        "    def call(self, inputs, training):\n",
        "        x = self.embedding(inputs)\n",
        "        for i in range(self.num_blocks):\n",
        "            x = self.transformer_blocks[i](x, training)\n",
        "        x = self.flatten(x)\n",
        "        return self.classifier(x)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Dataset splitting in Training/testing\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "# EMOTHAW dataset\n",
        "def generate_sample_data(num_samples, seq_len, num_features, num_classes):\n",
        "    data = np.random.rand(num_samples, seq_len, num_features)\n",
        "    labels = np.random.randint(0, num_classes, num_samples)\n",
        "    return data, labels\n",
        "\n",
        "# Sample of EMOTHAW dataset\n",
        "num_samples = 1000\n",
        "seq_length = 50\n",
        "num_features = 128\n",
        "num_classes = 3  # Depression, Anxiety and Stress\n",
        "\n",
        "data, labels = generate_mock_data(num_samples, seq_length, num_features, num_classes)\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(data, labels, test_size=0.2, random_state=42)\n",
        "\n",
        "# Build and compile the model\n",
        "num_blocks = 3\n",
        "embed_dim = 64\n",
        "num_heads = 4\n",
        "ff_dim = 128\n",
        "learning_rate = 0.001\n",
        "\n",
        "transformer_model = TransformerClassifier(num_classes, embed_dim, num_heads, ff_dim, num_blocks)\n",
        "transformer_model.compile(optimizer=Adam(learning_rate), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Training the model\n",
        "epochs = 25\n",
        "batch_size = 32\n",
        "\n",
        "transformer_model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size, validation_data=(X_test, y_test))\n",
        "\n",
        "# Evaluating the model\n",
        "loss, accuracy = transformer_model.evaluate(X_test, y_test)\n",
        "print(f\"Test Accuracy: {accuracy * 100:.2f}%\")\n"
      ],
      "metadata": {
        "id": "cm7DH_iac4G5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "from keras import Sequential\n",
        "from tensorflow import keras\n",
        "import os\n",
        "\n",
        "\n",
        "model = tf.keras.Sequential()\n",
        "model.add(tf.keras.layers.Flatten(input_shape=(28, 28)))\n",
        "model.add(tf.keras.layers.Dense(128, activation='relu'))\n",
        "model.add(tf.keras.layers.Dense(128, activation='relu'))\n",
        "model.add(tf.keras.layers.Dense(10, activation='softmax'))\n",
        "\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "model.fit(x_train, y_train, epochs=3)\n",
        "#model.save('Emotion-detection.model')\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7QWP_ECdgsP3",
        "outputId": "243dee9d-0ce6-4952-c96d-eb12f9329228"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/3\n",
            "1875/1875 [==============================] - 7s 3ms/step - loss: 0.2673 - accuracy: 0.9204\n",
            "Epoch 2/3\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.1084 - accuracy: 0.9667\n",
            "Epoch 3/3\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.0714 - accuracy: 0.9778\n"
          ]
        }
      ]
    }
  ]
}